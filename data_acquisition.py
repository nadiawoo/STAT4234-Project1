# -*- coding: utf-8 -*-
"""Data Acquisition

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RmI1z7tMN_kWxtz_CfMEot0WcLfsZsG0

###Testing Spotipy-facilitated API calls (did not work)
"""

!pip install spotipy #this is a lightweight Python library that allows us to access the Spotify web API with python

import spotipy
from spotipy.oauth2 import SpotifyClientCredentials
client_id = "846863b7c91a45e0b32f787dde9ae429"
client_secret = "3aa4a64d9ca64a3a8b429e07af58a8ca"
client_credentials_manager = SpotifyClientCredentials(client_id=client_id, client_secret=client_secret)
sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)

#testing if this works
name = ["Micheal Jackson","pitbull","Christina","Elvis Presley"] #using a manual list of artists
result = sp.search(name)

artists_uris = result['tracks']['items'][0]['artists'][0]['uri'] #this can be made into a loop for i in name

#Pull all of the artist's albums
artist_albums = sp.artist_albums(artists_uris, album_type='album')

#Store artist's albums' names' and uris in separate lists
artist_album_names = []
artist_album_uris = []
for i in range(len(artist_albums['items'])):
    artist_album_names.append(artist_albums['items'][i]['name'])
    artist_album_uris.append(artist_albums['items'][i]['uri'])

artist_album_names
artist_album_uris
#Keep names and uris in same order to keep track of duplicate albums

artist_album_names

#supposedly we now collected all of the artist of interest and their album name and uris, we can now retreive each song of album
def album_songs(uri):
    album = uri
    spotify_albums[album] = {}
    #Create keys-values of empty lists inside nested dictionary for album
    spotify_albums[album]['album'] = []
    spotify_albums[album]['track_number'] = []
    spotify_albums[album]['id'] = []
    spotify_albums[album]['name'] = []
    spotify_albums[album]['uri'] = []
    #pull data on album tracks
    tracks = sp.album_tracks(album)
    for n in range(len(tracks['items'])):
        spotify_albums[album]['album'].append(artist_album_names[album_count])
        spotify_albums[album]['track_number'].append(tracks['items'][n]['track_number'])
        spotify_albums[album]['id'].append(tracks['items'][n]['id'])
        spotify_albums[album]['name'].append(tracks['items'][n]['name'])
        spotify_albums[album]['uri'].append(tracks['items'][n]['uri'])

spotify_albums = {}
album_count = 0
for i in artist_album_uris: #each album
    album_songs(i)
    print(str(artist_album_names[album_count]) + " album songs has been added to spotify_albums dictionary")
    album_count+=1 #Updates album count once all tracks have been added

spotify_albums = {}
album_count = 0
for i in artist_album_uris: #each album
    album_songs(i)
    print(str(artist_album_names[album_count]) + " album songs has been added to spotify_albums dictionary")
    album_count+=1 #Updates album count once all tracks have been added

"""**Instead of choosing a list of artists, with the code below I am trying an alternative way to get as much artists as possible**"""

import spotipy
from spotipy.oauth2 import SpotifyClientCredentials
client_credentials_manager = SpotifyClientCredentials(client_id=client_id, client_secret=client_secret)
sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)


#testing
results = sp.search(q="taylor", type="track", limit=1)
track = results["tracks"]["items"][0]
print("Song Name:", track["name"])
print("Artist:", track["artists"][0]["name"])
print("Spotify URL:", track["external_urls"]["spotify"])

import spotipy
from spotipy.oauth2 import SpotifyClientCredentials
import time
import pandas as pd

# Authenticate with Spotify API
auth_manager = SpotifyClientCredentials()
sp = spotipy.Spotify(auth_manager=auth_manager)

# Define genres of interest
genres = ["pop", "hip-hop", "rock", "jazz", "country", "electronic", "reggae", "blues", "r&b", "metal"]

# Define search prefixes to bypass Spotify's 1000-result limit
search_prefixes = list("ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789")  # A-Z, 0-9

# Store unique artist IDs to prevent duplicates
artist_ids = set()

# Loop through all genres
for genre in genres:
    print(f"üîç Searching for artists in the '{genre}' genre...")

    for prefix in search_prefixes:
        offset = 0

        while offset < 1000:  # Ensure we stay within Spotify's 1000-item limit
            query = f"genre:{genre} artist:{prefix}*"
            try:
                results = sp.search(q=query, type="artist", limit=50, offset=offset)
                artists = results["artists"]["items"]

                if not artists:  # Stop if no more artists are found
                    break

                for artist in artists:
                    artist_ids.add(artist["id"])  # Store unique artist IDs

                print(f"‚úÖ {genre} ({prefix}*): {len(artist_ids)} unique artists collected.")

                offset += 50  # Move to next page
                time.sleep(0.5)  # Prevent hitting API rate limits

            except Exception as e:
                print(f"‚ö†Ô∏è Error occurred at offset {offset} for {genre} ({prefix}*): {e}")
                break  # Stop on error and continue to next prefix

print(f"\nüéâ Total unique artists collected: {len(artist_ids)}") #due to search limit, it gotten to metal (9*): 88735

df = pd.DataFrame(artist_ids)

# Save as CSV
df.to_csv("spotify_artists_testing.csv", index=False)

artist_df = pd.read_csv("spotify_artists_all.csv")

# Ensure the correct column name
print("Column Names in CSV:", artist_df.columns.tolist())

# Extract only artist IDs as a list
artist_ids = artist_df["0"].dropna().unique().tolist()

len(artist_ids)

import spotipy
from spotipy.oauth2 import SpotifyClientCredentials
import pandas as pd
import time
import random
from tqdm import tqdm

# Spotify API credentials
client_id = "79ca2c801915487184f9d06993a76eb0"
client_secret = "63a590ad354c42f1ac766effc83f3076"

auth_manager = SpotifyClientCredentials(client_id=client_id, client_secret=client_secret)
sp = spotipy.Spotify(auth_manager=auth_manager)

# Load dataset
df = pd.read_csv("/content/drive/MyDrive/Project 1/data.csv")

# Ensure correct column extraction
if "id" not in df.columns:
    print("‚ùå Column 'id' not found. Available columns:", df.columns)
    exit()

song_ids = df["id"].dropna().unique().tolist()

# Set initial delay parameters
MIN_SLEEP = 1.5  # Minimum wait time between requests
MAX_SLEEP = 5.0  # Maximum wait time if approaching rate limits
current_sleep = MIN_SLEEP  # Start with the minimum delay

# Function to handle rate limits
def handle_rate_limit(exception):
    global current_sleep
    if exception.http_status == 429:
        # Get Spotify's recommended wait time
        print(f"‚ö†Ô∏è Rate limit reached! Waiting for {'15'} seconds...")
        time.sleep(15)  # Wait before retrying
        current_sleep = min(MAX_SLEEP, current_sleep + 1)  # Increase delay to reduce future limits
    else:
        print(f"‚ö†Ô∏è API Error: {exception}")
        time.sleep(5)  # Short delay before retrying

# Fetch available markets for each song
song_market_data = []

print(f"üåç Fetching available markets for {len(song_ids)} songs...")

for idx, song_id in enumerate(tqdm(song_ids, desc="Processing Songs", unit="song")):
    retries = 3  # Max retries per song
    while retries > 0:
        try:
            # Fetch track details
            track = sp.track(song_id)

            # Store available markets
            song_market_data.append({
                "song_id": song_id,
                "available_markets": ", ".join(track["available_markets"])
            })

            # Print progress every 100 songs
            if idx % 100 == 0:
                print(f"‚úÖ [{idx+1}/{len(song_ids)}] Processed: {track['name']} ({len(track['available_markets'])} markets)")

            # Auto-save progress every 25 songs
            if idx % 25 == 0 and idx > 0:
                temp_df = pd.DataFrame(song_market_data)
                temp_df.to_csv("spotify_songs_markets_backup.csv", index=False)
                print(f"üíæ Auto-saved progress at {idx} songs...")

            # Dynamically adjust sleep time
            time.sleep(current_sleep)  # Prevent rate limits by slowing down requests
            break  # Exit retry loop

        except spotipy.exceptions.SpotifyException as e:
            handle_rate_limit(e)
            retries -= 1
        except Exception as e:
            print(f"‚ö†Ô∏è General Error for {song_id}: {e}")
            retries -= 1
            time.sleep(5)

print(f"\n‚úÖ Successfully fetched available markets for {len(song_market_data)} songs.")

# Convert to DataFrame
market_df = pd.DataFrame(song_market_data)

# Merge available markets with the existing dataset
df = df.merge(market_df, on="id", how="left")

# Save the updated dataset
df.to_csv("spotify_songs_with_markets.csv", index=False)

# Display dataset
import ace_tools as tools
tools.display_dataframe_to_user(name="Spotify Songs with Available Markets", dataframe=df)

"""**Extracting all artists is not a viable option considering the strict API call time limitations of Spotify, the intended library, and the time restraint of this project. Thus a pre-acquired Spotify dataset will be used: https://www.kaggle.com/datasets/rodolfofigueroa/spotify-12m-songs**

### Getting spotify_df on Kaggle

Downloading the 1.2 Million + spotify dataset on Kaggle: https://www.kaggle.com/datasets/rodolfofigueroa/spotify-12m-songs/data

###Wesbcraping for Artist's Grammy Awards and Nominations -> grammy dataframe
"""

import requests
import pandas as pd
from bs4 import BeautifulSoup
import re

# Wikipedia page URL
url = "https://en.wikipedia.org/wiki/List_of_American_Grammy_Award_winners_and_nominees"

# Get the webpage content
response = requests.get(url)
soup = BeautifulSoup(response.text, "html.parser")

# Find the Grammy winners' table
tables = soup.find_all("table", {"class": "wikitable"})
if not tables:
    print("‚ùå No Grammy winners table found.")
    exit()

table = tables[0]  # Assuming the first table is correct

# Extract table rows
rows = table.find_all("tr")

# Initialize data storage
grammy_data = []

# Process table rows (skip header row)
for row in rows[1:]:
    cols = row.find_all(["th", "td"])
    if len(cols) < 3:
        continue  # Skip invalid rows

    # Extract text and clean it
    raw_artists = cols[0].get_text("\n", strip=True)  # Keep new lines as separators
    awards = cols[1].get_text(strip=True)
    nominations = cols[2].get_text(strip=True)

    # Remove unwanted references (e.g., [233], [note 2])
    raw_artists = re.sub(r"\[\d+\]|\[note \d+\]", "", raw_artists)  # Cleans out Wikipedia references

    # Ensure no leftover brackets or numbers
    raw_artists = re.sub(r"[\[\]]", "", raw_artists)  # Removes stray brackets

    # Split artists correctly based on new lines
    artists = raw_artists.split("\n")

    # Store cleaned data
    for artist in artists:
        artist = artist.strip()
        if artist and not artist.isnumeric():  # Exclude stray numbers
            grammy_data.append((artist, awards, nominations))

#moving onto the second table
table = tables[1]

rows = table.find_all("tr")

for row in rows[1:]:
    cols = row.find_all(["th", "td"])
    if len(cols) < 3:
        continue  # Skip invalid rows

    # Extract text and clean it
    raw_artists = cols[0].get_text("\n", strip=True)  # Keep new lines as separators
    awards = cols[1].get_text(strip=True)
    nominations = cols[2].get_text(strip=True)

    # Remove unwanted references (e.g., [233], [note 2])
    raw_artists = re.sub(r"\[\d+\]|\[note \d+\]", "", raw_artists)  # Cleans out Wikipedia references

    # Ensure no leftover brackets or numbers
    raw_artists = re.sub(r"[\[\]]", "", raw_artists)  # Removes stray brackets

    # Split artists correctly based on new lines
    artists = raw_artists.split("\n")

    # Store cleaned data
    for artist in artists:
        artist = artist.strip()
        if artist and not artist.isnumeric():  # Exclude stray numbers
            grammy_data.append((artist, awards, nominations))

# Convert to DataFrame
df = pd.DataFrame(grammy_data, columns=["Artist", "Grammy Awards", "Grammy Nominations"])

# Save to CSV
df.to_csv("grammy_winners_cleaned.csv", index=False)